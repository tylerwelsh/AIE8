<aside>

# **ğŸ¬ Live Session Resources, recorded September 18th**

ğŸ–¼ï¸ [Slides](https://www.canva.com/design/DAGzMO1y0FQ/oJaw4HMIFecP3oX9jSO4fw/edit?utm_content=DAGzMO1y0FQ&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton) & ğŸ¥ [Recording](https://us02web.zoom.us/rec/share/jEs9TS_re1f9X3y2T61Dgv_bEp6EmVzVkiYDOC-cEU8WA2tR5jMI1bwsn4L_Al1n.msDqlCRCROFBaRCH)(78y?PRTg)

ğŸ§‘â€ğŸ’» [GitHub Repo](https://github.com/AI-Maker-Space/AIE8/tree/main/04_Production_RAG)

ğŸ“ [Homework](https://forms.gle/i2SdxgWX4ahFwNrCA)

ğŸ—£ï¸ [Feedback](https://forms.gle/ymYqK5MBLAG11jDB9)

</aside>

# Overview

In Session 4, we dig into LangChain and LangGraph!  LangChain has emerged as the leader of the pack of infrastructure [orchestration tools](https://github.com/a16z-infra/llm-app-stack?tab=readme-ov-file#orchestrators).

> LangChainÂ is a framework for developing applications powered by large language models (LLMs). [[Ref](https://python.langchain.com/docs/introduction/)]
> 

LangChain, as a company, has built an entire ecosystem of tools (weâ€™ll often refer to this as the Lang-X ecosystem).  

During this session, weâ€™ll learn about LangChain, LangGraph, and LangSmith. LangChain and LangGraph are used for orchestration (we can think of LangGraph as the next evolution of LangChain, well-suited for agents and agentic reasoning loops). 

> LangGraph is built for developers who want to build powerful, adaptable AI agents. [[Ref](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/)]
> 

LangSmith, on the other hand, is used for monitoring, visibility, logging, and evaluation. 

> **LangSmith**Â is a platform for building production-grade LLM applications. [[Ref](https://docs.smith.langchain.com/)]
> 

During Session 4, we will also cover a specific use case that weâ€™ll build on in the next session. From prototype to production with LangChain, letâ€™s gooooooo!

![image.png](attachment:f191d2c8-2555-48b0-98c6-6d54db14ed5d:image.png)

# ğŸ“›Â Required **Tooling & Account Setup**

1. Create an ollama account
    
    [Sign in to Ollama](https://ollama.com/signin)
    
2. [Optional] Create a LangSmith account ğŸ‘‡
    
    [LangSmith](https://smith.langchain.com/)
    

# ğŸ§‘â€ğŸ’»Â Recommended Pre-Work!

In this class, weâ€™ll learn each of the key constructs of LangChain, and weâ€™ll build and evaluate a RAG application with LangChain. Hereâ€™s what to do to prepare:

1. ğŸ«Â During this session, weâ€™ll begin exploration of our cohortâ€™s use case, inspired by the many Deep Research applications that every model provider has rolled out in 2025! During Cohort 8, weâ€™ll build our own Deep Research application to help us answer the question `â€œWhat should we build and why?â€`
    1. **Problem:** It is consistently challenging for individuals and companies to figure out what AI projects they should build first within their companies to maximize ROI and/or value created for themselves or their communities
    2. **Why:** 
        1. As the technology evolves, the use cases that it is best suited for evolve; that is, as automation improves, the potential body of use cases changes in concert with improvements in production LLM tooling
        2. As students in The AI Engineering Bootcamp, many of you still arenâ€™t sure what you should build, ship, and share for Demo Day! This is true despite having access to GPTs such as the [BuildğŸ—-ShipğŸš¢-Share ğŸš€ GPT](https://chatgpt.com/g/g-UwSQ1o8AW-build-ship-share-gpt?model=gpt-4o) and [ChatGPT Use Cases for Work](https://chatgpt.com/g/g-h5aUtVu0G-chatgpt-use-cases-for-work?locale=en-US&model=gpt-5-thinking).
    3. **Success:** Each student has multiple interesting use cases to build that serve either their company, themselves, or their communities
    - ğŸ§ªÂ Hypothesis: The existing tooling used for search, retrieval, and generation of helpful and useful answers to customer inquiries does not rapidly accelerate an agentâ€™s ability to deal with a large volume of customer complaints per day.
    1. **Audience:** We are building this application for individuals within the current cohort
    2. **Solution**: We want to build a solution that can answer questions like:
        
        
        | Category | Scenario / Question |
        | --- | --- |
        | Enterprise use | `How do people use AI in their daily work?` |
        | Enterprise use | `What are the most common ways people use AI in their work?` |
        | Personal use | `Do people use AI for their personal lives?"` |
        | Personal use | `What concerns or challenges do people have when using AI?` |
        | Community use | `What should I build to add value to the local communities Iâ€™m engaged in?` |
        
        **To test this use case**, we should try answering these questions using:
        
        1. ChatGPT out of the box
        2. The [BuildğŸ—-ShipğŸš¢-Share ğŸš€ GPT](https://chatgpt.com/g/g-UwSQ1o8AW-build-ship-share-gpt?model=gpt-4o)
        3. [ChatGPT Use Cases for Work](https://chatgpt.com/g/g-h5aUtVu0G-chatgpt-use-cases-for-work?locale=en-US&model=gpt-5-thinking)
        
        **To build our first simple RAG prototype**, we will use the following **use case data** from OpenAI
        
        [How people are using ChatGPT](https://openai.com/index/how-people-are-using-chatgpt/)
        
        In the future, we will add additional resources like [Anthropicâ€™s Economic Index Report](https://www.anthropic.com/research/anthropic-economic-index-september-2025-report) and others as they come out!
        
        We will *build upon this use case in the upcoming sessions (e.g., agents)* as well ğŸš€
        
2. The purpose of LangGraph (and subsequently, [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/)) becomes apparent the more you study the idea of state machines. As an introduction, check out CEO Harrison Chaseâ€™s TED Talk on Cognitive architectures.
    
    [The magical AI assistants of the future â€” and the engineering behind them](https://www.ted.com/talks/harrison_chase_the_magical_ai_assistants_of_the_future_and_the_engineering_behind_them?subtitle=en)
    
3. ğŸ¤“Â Check out additional reading material in [Go Deeper](https://www.notion.so/Session-4-RAG-with-LangGraph-OSS-Local-Models-Eval-w-LangSmith-26acd547af3d80838d5beba464d7e701?pvs=21).

# â›“ï¸Â Core Constructs: LangChain

When orchestrating complex LLM applications that leverage context and reasoning,  LangChain has emerged as a best-practice tool.  To begin, itâ€™s instructive to start with how we can leverage the pattern of Retrieval Augmented Generation to build apps using LangChain.

What are the core components we need to understand?

Let us start, where we should, with the abstraction of the `chain` in LangChain.

<aside>
â›“ï¸ Chain: a sequence of calls to other components

</aside>

With this idea of chains in mind, we can understand that `LangChain Expression Language, or LCEL`, allows us to ***compose chains*** easily.  LCEL also enables us to build production-grade prototypes that can be deployed with no code changes.

**A Simple Chain - Chat Models and Prompts**

The first chain weâ€™ll build with LCEL combines `Model` and `Prompt` from Models I/O.

Using LLM models that have been instruction-tuned and fine-tuned for chat is a best practice for LLM-powered applications.  Just as we learned about the {System, User, Assistant} roles in OpenAI, we can leverage chat-style models using the {System, Human, AI} roles in LangChain.  These are, for all intents and purposes, the same.

![Chat completions roles, OpenAI vs. LangChain.](attachment:d485df7c-ebb4-4099-ae60-3e4dfd6178d8:image.png)

Chat completions roles, OpenAI vs. LangChain.

*** Recall that with o1 models and newer,Â `developer`Â messages replace previousÂ `system`messages.*

Building our first LLM chain requires a chat prompt template.  We have seen this in our Pythonic RAG system, and can see many other examples [here](https://python.langchain.com/docs/how_to/#prompt-templates).  Here is what our first chain looks like in code.

> `chain = chat_prompt | openai_chat_model`
> 

**Our Second Chain - Retrieval Augmented Generation**

To do RAG with LangChain, we leverage the same process we used to build our Pythonic RAG application.  Below these steps are outlined with the relevant LangChain constructs and documentation that weâ€™ll use during Session 4!

1. Create **Database**
    1. `Document Loader` [[Ref](https://python.langchain.com/docs/how_to/#document-loaders)]
    2. `Text Splitter` [[Ref](https://python.langchain.com/docs/how_to/#text-splitters)]
    3. `Embedding Model` [[Ref](https://python.langchain.com/docs/integrations/text_embedding/)]
    4. `Vector Store` [[Ref](https://python.langchain.com/docs/how_to/#vector-stores)]
2. Ask **Question**
    1. `Embedding Model` [[Ref](https://python.langchain.com/docs/integrations/text_embedding/)]
3. Find **References**
    1. `Retriever` [[Ref](https://python.langchain.com/docs/how_to/#retrievers)]
4. **Augment** the Prompt
    1. `Prompt` [[Ref](https://python.langchain.com/docs/how_to/#prompt-templates)]
5. **Generate** a better answer!
    1. `Model` [[Ref](https://python.langchain.com/docs/integrations/chat/)]

<aside>
â›“ï¸ In the end, we want to create a single RAG chain that leverages LCEL

</aside>

Hereâ€™s an example in code of such a RAG chainğŸ‘‡

```python
simple_rag  = (
    {"context": retriever, "query": RunnablePassthrough()}
    | chat_prompt
    | openai_chat_model
    | StrOutputParser()
)
```

Notice that we see the word â€œRunnableâ€ in the code.

# **The Runnable**

 In LangChain, a Runnable is like a LEGO brick in your AI application - it's a standardized component that can be easily connected with other components. The real power of Runnables comes from their ability to be combined in flexible ways using LCEL (LangChain Expression Language).

*Every component of a chain is a **runnable**.*  

In fact, **the primary abstraction in the LangChain ecosystem is the runnable**.

> We often refer to aÂ `Runnable`Â created using LCEL as a "chain" [[Ref](https://python.langchain.com/docs/concepts/lcel/)]
> 

## **Key Features of Runnables**

**1. Universal Interface**

Every Runnable in LangChain follows the same pattern:

- Takes an input
- Performs some operation
- Returns an output

This consistency means you can treat different components (like models, retrievers, or parsers) in the same way.

**2. Built-in Parallelization**

Runnables come with methods for handling multiple inputs efficiently:

```python
# Process inputs in parallel, maintain order
results = chain.batch([input1, input2, input3])

# Process inputs as they complete
for result in chain.batch_as_completed([input1, input2, input3]):
    print(result)
```

**3. Streaming Support**

Perfect for responsive applications:

```python
# Stream outputs as they're generated
for chunk in chain.stream({"query": "Tell me a story"}):
    print(chunk, end="", flush=True)
```

**4. Easy Composition**

TheÂ `|`Â operator makes building pipelines intuitive, as seen above! e.g.;

```python
# Create a basic RAG chain
rag_chain = retriever | prompt | model | output_parser
```

**Common Types of Runnables**

- Language Models: Like ourÂ `ChatOpenAI`Â instance
- Prompt Templates: Format inputs consistently
- Retrievers: Get relevant context from a vector store
- Output Parsers: Structure model outputs
- LangGraph Nodes: Individual components in our graph

Think of Runnables as the building blocks of your LLM application. Just like how you can combine LEGO bricks in countless ways, you can mix and match Runnables to create increasingly sophisticated applications!

# ğŸ•¸ï¸Â  Core Constructs: LangGraph

Instead of building our entire RAG chain using runnables, the best-practice in 2025 is to use LangGraph directly.

<aside>
ğŸ”„ LangGraph lets us **add cycles** to applications built on LangChain.

</aside>

The essence of LangGraph is that it uses graphs to add cycles.  

**Why Cycles?**

We can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application ***agent-forward*** while still giving the powerful functionality of traditional loops.

Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.

**Why LangGraph?**

During this session, *we will be using LangGraph as a Directed Acyclic Graph (DAG*).  Beyond the agent-forward approach - we can easily compose and combine traditional DAG chains with powerful cyclic behavior due to the tight integration with LCEL. 

In this way, LangGraph is a natural extension to LangChain's core offerings!

## Graphs

Graphs are collections of connected objects: nodes and edges.

- **Node**: Think `function` or `runnable`; i.e. *something that changes **state***
- **Edge**: Think path to take; i.e., *where to pass **state** object next*

A state object is initially defined by passing a state definition to a class representing the graph.  This state object, or `StateGraph`, gets updated over time.  The agent's internal state is represented simply as a list of messages.  Remember how we interacted with the OpenAI API with a list of messages with roles?  Same idea.

Just as every component of a chain is a runnable, each node in our graph can be a runnable, or even an entire chain!  

Welcome to the next layer of abstraction.

# âš’ï¸Â LangSmith for Evaluation & Visibility

> **LangSmith**Â is a platform for building production-grade LLM applications. [[Ref](https://docs.smith.langchain.com/)]
> 

In addition to building out a RAG application with LangChain, we will also learn what the developer sees behind the scenes when users interact with our LangChain application!  Enter â€¦ LangSmith.

> â€œLangSmith turns LLM â€œmagicâ€ into enterprise-ready applications.â€
> 

Ultimately, we will use LangSmith for evaluation and for visibility.

**Evaluation**

The key to understanding the right way to do quantitative evaluation with tools like LangSmith is Metrics Driven Development (MDD). There are three steps to keep in mind:

1. Establish a baseline
2. **Change stuff** that potentially improves baseline
3. **Recalculate** metrics

Itâ€™s not about absolute values.  Rather, MDD is about *relative changes* in evaluation metrics.

We will explore LangSmith further in future sessions!

# ğŸ•³ï¸ Go Deeper!

- ğŸ¤–Â For todayâ€™s primary build, weâ€™ll be leveraging the open-source [gpt-oss](https://openai.com/index/introducing-gpt-oss/) model (August, 2025) that we did a deep dive on [here](https://www.youtube.com/live/nyb3TnUkwE8?si=Yz97mSOltslcPnIM). We will also be leveraging the newly-released [EmbeddingGemma](https://developers.googleblog.com/en/introducing-embeddinggemma/) model from Google (September, 2025).
- https://blog.langchain.com/the-rise-of-context-engineering/ (June 2025)
- [Is LangGraph used in Production?](https://blog.langchain.com/is-langgraph-used-in-production/) (Feb 2025)