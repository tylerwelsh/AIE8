{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity #1 — Experiment Results\n",
    "\n",
    "- Increase Parallelism (max_concurrent_research_units: 1 → 3)\n",
    "  - Reduced wall-clock time by running researchers concurrently, but increased token throughput and 429 rate-limit risk.\n",
    "  - Higher cost variability due to multiple simultaneous tool/model calls.\n",
    "\n",
    "- Deeper Research (max_researcher_iterations: 1 → 3; max_react_tool_calls: 2 → 5)\n",
    "  - Produced richer notes and a more comprehensive final report.\n",
    "  - Increased latency and token usage, with a higher chance of hitting context/token limits.\n",
    "\n",
    "- Use Anthropic Native Search (search_api: \"tavily\" → \"anthropic\")\n",
    "  - Fewer explicit tool messages and simpler loops, but more LLM-side token usage subject to Anthropic rate limits.\n",
    "  - Comparable quality if queries are clear, with variability tied to model-side retrieval quality.\n",
    "\n",
    "- Disable Clarification (allow_clarification: True → False)\n",
    "  - Avoided one initial model call and sped up start-up.\n",
    "  - Risked missing key constraints, occasionally reducing accuracy when the request is ambiguous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Open Deep Research - Supervisor-Researcher Architecture\n",
    "\n",
    "In this notebook, we'll explore the **supervisor-researcher delegation architecture** for conducting deep research with LangGraph.\n",
    "\n",
    "You can visit this repository to see the original application: [Open Deep Research](https://github.com/langchain-ai/open_deep_research)\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "This implementation uses a **hierarchical delegation pattern** where:\n",
    "\n",
    "1. **User Clarification** - Optionally asks clarifying questions to understand the research scope\n",
    "2. **Research Brief Generation** - Transforms user messages into a structured research brief\n",
    "3. **Supervisor** - A lead researcher that analyzes the brief and delegates research tasks\n",
    "4. **Parallel Researchers** - Multiple sub-agents that conduct focused research simultaneously\n",
    "5. **Research Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings are combined into a comprehensive report\n",
    "\n",
    "![Architecture Diagram](https://private-user-images.githubusercontent.com/181020547/465824799-12a2371b-8be2-4219-9b48-90503eb43c69.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjAwNDgyMzcsIm5iZiI6MTc2MDA0NzkzNywicGF0aCI6Ii8xODEwMjA1NDcvNDY1ODI0Nzk5LTEyYTIzNzFiLThiZTItNDIxOS05YjQ4LTkwNTAzZWI0M2M2OS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMDA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTAwOVQyMjEyMTdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iYTRmYTAzYjkzYjA2MGE4ZTZlYjQ4ODU1OWIwY2VlZWU0Mzk0YzdmMjQ1YTlhMDMyNmI3NWNlZTQxNDdlZGViJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.a8477QD1J4Lrmys7jB8gt_H5pdiKBsKsu3npEqZjEpo)\n",
    "\n",
    "This differs from a section-based approach by allowing dynamic task decomposition based on the research question, rather than predefined sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "You'll need API keys for Anthropic (for the LLM) and Tavily (for web search). We'll configure the system to use Anthropic's Claude Sonnet 4 exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: State Definitions\n",
    "\n",
    "The state structure is hierarchical with three levels:\n",
    "\n",
    "### Agent State (Top Level)\n",
    "Contains the overall conversation messages, research brief, accumulated notes, and final report.\n",
    "\n",
    "### Supervisor State (Middle Level)\n",
    "Manages the research supervisor's messages, research iterations, and coordinating parallel researchers.\n",
    "\n",
    "### Researcher State (Bottom Level)\n",
    "Each individual researcher has their own message history, tool call iterations, and research findings.\n",
    "\n",
    "We also have structured outputs for tool calling:\n",
    "- **ConductResearch** - Tool for supervisor to delegate research to a sub-agent\n",
    "- **ResearchComplete** - Tool to signal research phase is done\n",
    "- **ClarifyWithUser** - Structured output for asking clarifying questions\n",
    "- **ResearchQuestion** - Structured output for the research brief\n",
    "\n",
    "Let's import these from our library: [`open_deep_library/state.py`](open_deep_library/state.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import state definitions from the library\n",
    "from open_deep_library.state import (\n",
    "    # Main workflow states\n",
    "    AgentState,           # Lines 65-72: Top-level agent state with messages, research_brief, notes, final_report\n",
    "    AgentInputState,      # Lines 62-63: Input state is just messages\n",
    "    \n",
    "    # Supervisor states\n",
    "    SupervisorState,      # Lines 74-81: Supervisor manages research delegation and iterations\n",
    "    \n",
    "    # Researcher states\n",
    "    ResearcherState,      # Lines 83-90: Individual researcher with messages and tool iterations\n",
    "    ResearcherOutputState, # Lines 92-96: Output from researcher (compressed research + raw notes)\n",
    "    \n",
    "    # Structured outputs for tool calling\n",
    "    ConductResearch,      # Lines 15-19: Tool for delegating research to sub-agents\n",
    "    ResearchComplete,     # Lines 21-22: Tool to signal research completion\n",
    "    ClarifyWithUser,      # Lines 30-41: Structured output for user clarification\n",
    "    ResearchQuestion,     # Lines 43-48: Structured output for research brief\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question 1:\n",
    "\n",
    " Explain the interrelationships between the three states.  Why don't we just make a single huge state?\n",
    "\n",
    "### ✅ ANSWER:\n",
    "Agent state orchestrates everything. The flow looks like Agent->Superviso->Reseacrher(s)->supervisor->agent.\n",
    "The relationship is that they are all working on their specialized subtasks, while be orchestrated by the agent state. \n",
    "\n",
    "This is not one large state because we want to promote modularity and parallelism of each state. It simplifies the graph allowing each subgraph to be specialized in something. This produces better quality outputs using states rather than one large on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Utility Functions and Tools\n",
    "\n",
    "The system uses several key utilities:\n",
    "\n",
    "### Search Tools\n",
    "- **tavily_search** - Async web search with automatic summarization to stay within token limits\n",
    "- Supports Anthropic native web search and Tavily API\n",
    "\n",
    "### Reflection Tools\n",
    "- **think_tool** - Allows researchers to reflect on their progress and plan next steps (ReAct pattern)\n",
    "\n",
    "### Helper Utilities\n",
    "- **get_all_tools** - Assembles the complete toolkit (search + MCP + reflection)\n",
    "- **get_today_str** - Provides current date context for research\n",
    "- Token limit handling utilities for graceful degradation\n",
    "\n",
    "These are defined in [`open_deep_library/utils.py`](open_deep_library/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions and tools from the library\n",
    "from open_deep_library.utils import (\n",
    "    # Search tool - Lines 43-136: Tavily search with automatic summarization\n",
    "    tavily_search,\n",
    "    \n",
    "    # Reflection tool - Lines 219-244: Strategic thinking tool for ReAct pattern\n",
    "    think_tool,\n",
    "    \n",
    "    # Tool assembly - Lines 569-597: Get all configured tools\n",
    "    get_all_tools,\n",
    "    \n",
    "    # Date utility - Lines 872-879: Get formatted current date\n",
    "    get_today_str,\n",
    "    \n",
    "    # Supporting utilities for error handling\n",
    "    get_api_key_for_model,          # Lines 892-914: Get API keys from config or env\n",
    "    is_token_limit_exceeded,         # Lines 665-701: Detect token limit errors\n",
    "    get_model_token_limit,           # Lines 831-846: Look up model's token limit\n",
    "    remove_up_to_last_ai_message,    # Lines 848-866: Truncate messages for retry\n",
    "    anthropic_websearch_called,      # Lines 607-637: Detect Anthropic native search usage\n",
    "    openai_websearch_called,         # Lines 639-658: Detect OpenAI native search usage\n",
    "    get_notes_from_tool_calls,       # Lines 599-601: Extract notes from tool messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question 2:  \n",
    "\n",
    "What are the advantages and disadvantages of importing these components instead of including them in the notebook?\n",
    "\n",
    "### ANSWER ✅:\n",
    "Advantages:\n",
    "- Keeps the notebook short and concise\n",
    "- Ensures a single source of truth, all these methods live elsewhere.\n",
    "- All of it is easily reusable and can be imported elsewhere (another notebook for instance). \n",
    "- On reusability, we could also test these methods since they're in a module instead of within the notebook\n",
    "\n",
    "Disadvantages:\n",
    "- The notebook itself becomes harder to tweak. If I wanted to inline change something in one of the methods, I need to make the changes and reimport, vs jsut inlinin the change if it lived within the notebook\n",
    "- In my opinion, it adds friction importing all of the methods as a package vs the code just all living in the notebook.\n",
    "    - Though, I do prefer the module method being used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Configuration System\n",
    "\n",
    "The configuration system controls:\n",
    "\n",
    "### Research Behavior\n",
    "- **allow_clarification** - Whether to ask clarifying questions before research\n",
    "- **max_concurrent_research_units** - How many parallel researchers can run (default: 5)\n",
    "- **max_researcher_iterations** - How many times supervisor can delegate research (default: 6)\n",
    "- **max_react_tool_calls** - Tool call limit per researcher (default: 10)\n",
    "\n",
    "### Model Configuration\n",
    "- **research_model** - Model for research and supervision (we'll use Anthropic)\n",
    "- **compression_model** - Model for synthesizing findings\n",
    "- **final_report_model** - Model for writing the final report\n",
    "- **summarization_model** - Model for summarizing web search results\n",
    "\n",
    "### Search Configuration\n",
    "- **search_api** - Which search API to use (ANTHROPIC, TAVILY, or NONE)\n",
    "- **max_content_length** - Character limit before summarization\n",
    "\n",
    "Defined in [`open_deep_library/configuration.py`](open_deep_library/configuration.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration from the library\n",
    "from open_deep_library.configuration import (\n",
    "    Configuration,    # Lines 38-247: Main configuration class with all settings\n",
    "    SearchAPI,        # Lines 11-17: Enum for search API options (ANTHROPIC, TAVILY, NONE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Prompt Templates\n",
    "\n",
    "The system uses carefully engineered prompts for each phase:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "**clarify_with_user_instructions** - Analyzes if the research scope is clear or needs clarification\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "**transform_messages_into_research_topic_prompt** - Converts user messages into a detailed research brief\n",
    "\n",
    "### Phase 3: Supervisor\n",
    "**lead_researcher_prompt** - System prompt for the supervisor that manages delegation strategy\n",
    "\n",
    "### Phase 4: Researcher\n",
    "**research_system_prompt** - System prompt for individual researchers conducting focused research\n",
    "\n",
    "### Phase 5: Compression\n",
    "**compress_research_system_prompt** - Prompt for synthesizing research findings without losing information\n",
    "\n",
    "### Phase 6: Final Report\n",
    "**final_report_generation_prompt** - Comprehensive prompt for writing the final report\n",
    "\n",
    "All prompts are defined in [`open_deep_library/prompts.py`](open_deep_library/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt templates from the library\n",
    "from open_deep_library.prompts import (\n",
    "    clarify_with_user_instructions,                    # Lines 3-41: Ask clarifying questions\n",
    "    transform_messages_into_research_topic_prompt,     # Lines 44-77: Generate research brief\n",
    "    lead_researcher_prompt,                            # Lines 79-136: Supervisor system prompt\n",
    "    research_system_prompt,                            # Lines 138-183: Researcher system prompt\n",
    "    compress_research_system_prompt,                   # Lines 186-222: Research compression prompt\n",
    "    final_report_generation_prompt,                    # Lines 228-308: Final report generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Node Functions - The Building Blocks\n",
    "\n",
    "Now let's look at the node functions that make up our graph. We'll import them from the library and understand what each does.\n",
    "\n",
    "### The Complete Research Workflow\n",
    "\n",
    "The workflow consists of 8 key nodes organized into 3 subgraphs:\n",
    "\n",
    "1. **Main Graph Nodes:**\n",
    "   - `clarify_with_user` - Entry point that checks if clarification is needed\n",
    "   - `write_research_brief` - Transforms user input into structured research brief\n",
    "   - `final_report_generation` - Synthesizes all research into final report\n",
    "\n",
    "2. **Supervisor Subgraph Nodes:**\n",
    "   - `supervisor` - Lead researcher that plans and delegates\n",
    "   - `supervisor_tools` - Executes supervisor's tool calls (delegation, reflection)\n",
    "\n",
    "3. **Researcher Subgraph Nodes:**\n",
    "   - `researcher` - Individual researcher conducting focused research\n",
    "   - `researcher_tools` - Executes researcher's tool calls (search, reflection)\n",
    "   - `compress_research` - Synthesizes researcher's findings\n",
    "\n",
    "All nodes are defined in [`open_deep_library/deep_researcher.py`](open_deep_library/deep_researcher.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: clarify_with_user\n",
    "\n",
    "**Purpose:** Analyzes user messages and asks clarifying questions if the research scope is unclear.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check if clarification is enabled in configuration\n",
    "2. Use structured output to analyze if clarification is needed\n",
    "3. If needed, end with a clarifying question for the user\n",
    "4. If not needed, proceed to research brief with verification message\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 60-115](open_deep_library/deep_researcher.py#L60-L115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clarify_with_user node\n",
    "from open_deep_library.deep_researcher import clarify_with_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: write_research_brief\n",
    "\n",
    "**Purpose:** Transforms user messages into a structured research brief for the supervisor.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Use structured output to generate detailed research brief from messages\n",
    "2. Initialize supervisor with system prompt and research brief\n",
    "3. Set up supervisor messages with proper context\n",
    "\n",
    "**Why this matters:** A well-structured research brief helps the supervisor make better delegation decisions.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 118-175](open_deep_library/deep_researcher.py#L118-L175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the write_research_brief node\n",
    "from open_deep_library.deep_researcher import write_research_brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: supervisor\n",
    "\n",
    "**Purpose:** Lead research supervisor that plans research strategy and delegates to sub-researchers.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure model with three tools:\n",
    "   - `ConductResearch` - Delegate research to a sub-agent\n",
    "   - `ResearchComplete` - Signal that research is done\n",
    "   - `think_tool` - Strategic reflection before decisions\n",
    "2. Generate response based on current context\n",
    "3. Increment research iteration count\n",
    "4. Proceed to tool execution\n",
    "\n",
    "**Decision Making:** The supervisor uses `think_tool` to reflect before delegating research, ensuring thoughtful decomposition of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 178-223](open_deep_library/deep_researcher.py#L178-L223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor node (from supervisor subgraph)\n",
    "from open_deep_library.deep_researcher import supervisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: supervisor_tools\n",
    "\n",
    "**Purpose:** Executes the supervisor's tool calls, including strategic thinking and research delegation.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check exit conditions:\n",
    "   - Exceeded maximum iterations\n",
    "   - No tool calls made\n",
    "   - `ResearchComplete` called\n",
    "2. Process `think_tool` calls for strategic reflection\n",
    "3. Execute `ConductResearch` calls in parallel:\n",
    "   - Spawn researcher subgraphs for each delegation\n",
    "   - Limit to `max_concurrent_research_units` (default: 5)\n",
    "   - Gather all results asynchronously\n",
    "4. Aggregate findings and return to supervisor\n",
    "\n",
    "**Parallel Execution:** This is where the magic happens - multiple researchers work simultaneously on different aspects of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 225-349](open_deep_library/deep_researcher.py#L225-L349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor_tools node\n",
    "from open_deep_library.deep_researcher import supervisor_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: researcher\n",
    "\n",
    "**Purpose:** Individual researcher that conducts focused research on a specific topic.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load all available tools (search, MCP, reflection)\n",
    "2. Configure model with tools and researcher system prompt\n",
    "3. Generate response with tool calls\n",
    "4. Increment tool call iteration count\n",
    "\n",
    "**ReAct Pattern:** Researchers use `think_tool` to reflect after each search, deciding whether to continue or provide their answer.\n",
    "\n",
    "**Available Tools:**\n",
    "- Search tools (Tavily or Anthropic native search)\n",
    "- `think_tool` for strategic reflection\n",
    "- `ResearchComplete` to signal completion\n",
    "- MCP tools (if configured)\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 365-424](open_deep_library/deep_researcher.py#L365-L424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher node (from researcher subgraph)\n",
    "from open_deep_library.deep_researcher import researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 6: researcher_tools\n",
    "\n",
    "**Purpose:** Executes the researcher's tool calls, including searches and strategic reflection.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check early exit conditions (no tool calls, native search used)\n",
    "2. Execute all tool calls in parallel:\n",
    "   - Search tools fetch and summarize web content\n",
    "   - `think_tool` records strategic reflections\n",
    "   - MCP tools execute external integrations\n",
    "3. Check late exit conditions:\n",
    "   - Exceeded `max_react_tool_calls` (default: 10)\n",
    "   - `ResearchComplete` called\n",
    "4. Continue research loop or proceed to compression\n",
    "\n",
    "**Error Handling:** Safely handles tool execution errors and continues with available results.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 435-509](open_deep_library/deep_researcher.py#L435-L509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher_tools node\n",
    "from open_deep_library.deep_researcher import researcher_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 7: compress_research\n",
    "\n",
    "**Purpose:** Compresses and synthesizes research findings into a concise, structured summary.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure compression model\n",
    "2. Add compression instruction to messages\n",
    "3. Attempt compression with retry logic:\n",
    "   - If token limit exceeded, remove older messages\n",
    "   - Retry up to 3 times\n",
    "4. Extract raw notes from tool and AI messages\n",
    "5. Return compressed research and raw notes\n",
    "\n",
    "**Why Compression?** Researchers may accumulate lots of tool outputs and reflections. Compression ensures:\n",
    "- All important information is preserved\n",
    "- Redundant information is deduplicated\n",
    "- Content stays within token limits for the final report\n",
    "\n",
    "**Token Limit Handling:** Gracefully handles token limit errors by progressively truncating messages.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 511-585](open_deep_library/deep_researcher.py#L511-L585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the compress_research node\n",
    "from open_deep_library.deep_researcher import compress_research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 8: final_report_generation\n",
    "\n",
    "**Purpose:** Generates the final comprehensive research report from all collected findings.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Extract all notes from completed research\n",
    "2. Configure final report model\n",
    "3. Attempt report generation with retry logic:\n",
    "   - If token limit exceeded, truncate findings by 10%\n",
    "   - Retry up to 3 times\n",
    "4. Return final report or error message\n",
    "\n",
    "**Token Limit Strategy:**\n",
    "- First retry: Use model's token limit × 4 as character limit\n",
    "- Subsequent retries: Reduce by 10% each time\n",
    "- Graceful degradation with helpful error messages\n",
    "\n",
    "**Report Quality:** The prompt guides the model to create well-structured reports with:\n",
    "- Proper headings and sections\n",
    "- Inline citations\n",
    "- Comprehensive coverage of all findings\n",
    "- Sources section at the end\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 607-697](open_deep_library/deep_researcher.py#L607-L697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final_report_generation node\n",
    "from open_deep_library.deep_researcher import final_report_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Graph Construction - Putting It All Together\n",
    "\n",
    "The system is organized into three interconnected graphs:\n",
    "\n",
    "### 1. Researcher Subgraph (Bottom Level)\n",
    "Handles individual focused research on a specific topic:\n",
    "```\n",
    "START → researcher → researcher_tools → compress_research → END\n",
    "               ↑            ↓\n",
    "               └────────────┘ (loops until max iterations or ResearchComplete)\n",
    "```\n",
    "\n",
    "### 2. Supervisor Subgraph (Middle Level)\n",
    "Manages research delegation and coordination:\n",
    "```\n",
    "START → supervisor → supervisor_tools → END\n",
    "            ↑              ↓\n",
    "            └──────────────┘ (loops until max iterations or ResearchComplete)\n",
    "            \n",
    "supervisor_tools spawns multiple researcher_subgraphs in parallel\n",
    "```\n",
    "\n",
    "### 3. Main Deep Researcher Graph (Top Level)\n",
    "Orchestrates the complete research workflow:\n",
    "```\n",
    "START → clarify_with_user → write_research_brief → research_supervisor → final_report_generation → END\n",
    "                 ↓                                       (supervisor_subgraph)\n",
    "               (may end early if clarification needed)\n",
    "```\n",
    "\n",
    "Let's import the compiled graphs from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-compiled graphs from the library\n",
    "from open_deep_library.deep_researcher import (\n",
    "    # Bottom level: Individual researcher workflow\n",
    "    researcher_subgraph,    # Lines 588-605: researcher → researcher_tools → compress_research\n",
    "    \n",
    "    # Middle level: Supervisor coordination\n",
    "    supervisor_subgraph,    # Lines 351-363: supervisor → supervisor_tools (spawns researchers)\n",
    "    \n",
    "    # Top level: Complete research workflow\n",
    "    deep_researcher,        # Lines 699-719: Main graph with all phases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Architecture?\n",
    "\n",
    "### Advantages of Supervisor-Researcher Delegation\n",
    "\n",
    "1. **Dynamic Task Decomposition**\n",
    "   - Unlike section-based approaches with predefined structure, the supervisor can break down research based on the actual question\n",
    "   - Adapts to different types of research (comparisons, lists, deep dives, etc.)\n",
    "\n",
    "2. **Parallel Execution**\n",
    "   - Multiple researchers work simultaneously on different aspects\n",
    "   - Much faster than sequential section processing\n",
    "   - Configurable parallelism (1-20 concurrent researchers)\n",
    "\n",
    "3. **ReAct Pattern for Quality**\n",
    "   - Researchers use `think_tool` to reflect after each search\n",
    "   - Prevents excessive searching and improves search quality\n",
    "   - Natural stopping conditions based on information sufficiency\n",
    "\n",
    "4. **Flexible Tool Integration**\n",
    "   - Easy to add MCP tools for specialized research\n",
    "   - Supports multiple search APIs (Anthropic, Tavily)\n",
    "   - Each researcher can use different tool combinations\n",
    "\n",
    "5. **Graceful Token Limit Handling**\n",
    "   - Compression prevents token overflow\n",
    "   - Progressive truncation in final report generation\n",
    "   - Research can scale to arbitrary depths\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Complexity:** More moving parts than section-based approach\n",
    "- **Cost:** Parallel researchers use more tokens (but faster)\n",
    "- **Unpredictability:** Research structure emerges dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Running the Deep Researcher\n",
    "\n",
    "Now let's see the system in action! We'll use it to analyze a PDF document about how people use AI.\n",
    "\n",
    "### Setup\n",
    "\n",
    "We need to:\n",
    "1. Load the PDF document\n",
    "2. Configure the execution with Anthropic settings\n",
    "3. Run the research workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF with 112460 characters\n",
      "First 500 characters:\n",
      "NBER WORKING PAPER SERIES\n",
      "HOW PEOPLE USE CHATGPT\n",
      "Aaron Chatterji\n",
      "Thomas Cunningham\n",
      "David J. Deming\n",
      "Zoe Hitzig\n",
      "Christopher Ong\n",
      "Carl Yan Shan\n",
      "Kevin Wadman\n",
      "Working Paper 34255\n",
      "http://www.nber.org/papers/w34255\n",
      "NATIONAL BUREAU OF ECONOMIC RESEARCH\n",
      "1050 Massachusetts Avenue\n",
      "Cambridge, MA 02138\n",
      "September 2025\n",
      "We acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan \n",
      "Beiermeister,  Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, \n",
      "Harrison Satcher,  Gawe...\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n",
    "def load_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Load and extract text from PDF.\"\"\"\n",
    "    pdf_text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_text += page.extract_text() + \"\\n\\n\"\n",
    "    return pdf_text\n",
    "\n",
    "# Load the PDF about how people use AI\n",
    "pdf_path = \"data/howpeopleuseai.pdf\"\n",
    "pdf_content = load_pdf(pdf_path)\n",
    "\n",
    "print(f\"Loaded PDF with {len(pdf_content)} characters\")\n",
    "print(f\"First 500 characters:\\n{pdf_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph ready for execution\n",
      "  (Note: The graph is pre-compiled from the library)\n"
     ]
    }
   ],
   "source": [
    "# Set up the graph with Anthropic configuration\n",
    "from IPython.display import Markdown, display\n",
    "import uuid\n",
    "\n",
    "# Note: deep_researcher is already compiled from the library\n",
    "# For this demo, we'll use it directly without additional checkpointing\n",
    "graph = deep_researcher\n",
    "\n",
    "print(\"✓ Graph ready for execution\")\n",
    "print(\"  (Note: The graph is pre-compiled from the library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Anthropic\n",
    "\n",
    "We'll configure the system to use:\n",
    "- **Claude Sonnet 4** for all research, supervision, and report generation\n",
    "- **Tavily** for web search (you can also use Anthropic's native search)\n",
    "- **Moderate parallelism** (3 concurrent researchers)\n",
    "- **Clarification enabled** (will ask if research scope is unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration ready\n",
      "  - Research Model: Claude Sonnet 4\n",
      "  - Max Concurrent Researchers: 3\n",
      "  - Max Iterations: 4\n",
      "  - Search API: Tavily\n"
     ]
    }
   ],
   "source": [
    "# Configure for Anthropic with moderate settings\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Model configuration - using Claude Sonnet 4 for everything\n",
    "        \"research_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"research_model_max_tokens\": 4100,\n",
    "        \n",
    "        \"compression_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"compression_model_max_tokens\": 4100,\n",
    "        \n",
    "        \"final_report_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"final_report_model_max_tokens\": 6000,\n",
    "        \n",
    "        \"summarization_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"summarization_model_max_tokens\": 4100,\n",
    "        \n",
    "        # Research behavior\n",
    "        \"allow_clarification\": True,\n",
    "        \"max_concurrent_research_units\": 1,  # 1 parallel researchers\n",
    "        \"max_researcher_iterations\": 2,      # Supervisor can delegate up to 2 times\n",
    "        \"max_react_tool_calls\": 3,           # Each researcher can make up to 3 tool calls\n",
    "        \n",
    "        # Search configuration\n",
    "        \"search_api\": \"tavily\",  # Using Tavily for web search\n",
    "        \"max_content_length\": 50000,\n",
    "        \n",
    "        # Thread ID for this conversation\n",
    "        \"thread_id\": str(uuid.uuid4())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  - Research Model: Claude Sonnet 4\")\n",
    "print(f\"  - Max Concurrent Researchers: 3\")\n",
    "print(f\"  - Max Iterations: 4\")\n",
    "print(f\"  - Search API: Tavily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Research\n",
    "\n",
    "Now let's run the research! We'll ask the system to analyze the PDF and provide insights about how people use AI.\n",
    "\n",
    "The workflow will:\n",
    "1. **Clarify** - Check if the request is clear (may skip if obvious)\n",
    "2. **Research Brief** - Transform our request into a structured brief\n",
    "3. **Supervisor** - Plan research strategy and delegate to researchers\n",
    "4. **Parallel Research** - Multiple researchers gather information simultaneously\n",
    "5. **Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings combined into comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting research workflow...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Node: clarify_with_user\n",
      "============================================================\n",
      "\n",
      "I have sufficient information to proceed with your analysis request. You've provided an NBER working paper titled \"How People Use ChatGPT\" and are asking for insights about: 1) main findings about AI usage patterns, 2) most common use cases, and 3) emerging trends from the data. Based on the excerpt provided, I can see this is a comprehensive study covering ChatGPT adoption from November 2022 through July 2025, including demographic patterns and usage classification. I will now begin analyzing the document content and preparing a detailed report addressing your three key questions.\n",
      "\n",
      "============================================================\n",
      "Node: write_research_brief\n",
      "============================================================\n",
      "\n",
      "Research Brief Generated:\n",
      "I need a comprehensive analysis of the NBER working paper \"How People Use ChatGPT\" by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman (Working Paper No. 34255, September 2025). Specifically, I want detailed insights on three key areas: 1) What are the main findings about how people are using AI/ChatGPT, including demographic patterns, adoption rates, and user characteristics; 2) What are the most common use cases identified in the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Node: research_supervisor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Node: final_report_generation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT GENERATED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Analysis of \"How People Use ChatGPT\" - NBER Working Paper 34255\n",
       "\n",
       "## Overview of the Study\n",
       "\n",
       "NBER Working Paper No. 34255, titled \"How People Use ChatGPT,\" represents one of the most comprehensive empirical studies of AI chatbot usage patterns to date. Published in September 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman, the study documents ChatGPT's remarkable consumer adoption journey from its November 2022 launch through July 2025 [1][2].\n",
       "\n",
       "The research employed a sophisticated privacy-preserving automated pipeline to analyze representative samples of ChatGPT conversations while maintaining strict user anonymity. Using a Data Clean Room (DCR), the researchers automatically scrubbed personally identifiable information and never allowed direct viewing of actual message content, instead relying on automated classifiers to categorize messages by purpose, topic, and intent [3]. This methodological approach, approved by Harvard IRB, ensures both research validity and user privacy protection.\n",
       "\n",
       "By July 2025, the study documented that ChatGPT had reached approximately 10% of the world's adult population, representing over 700 million weekly active users who collectively send more than 2.6 billion messages per day—equivalent to over 30,000 messages per second [2][4]. This scale of adoption represents an unprecedented rate of technology diffusion, with ChatGPT achieving in less than two years what took smartphones approximately ten years to accomplish.\n",
       "\n",
       "## Main Findings About AI/ChatGPT Usage Patterns\n",
       "\n",
       "### Explosive Growth and Adoption Metrics\n",
       "\n",
       "The study reveals extraordinary growth trajectories that redefine our understanding of technology adoption curves. ChatGPT reached one million users within just five days of its launch on December 5, 2022, and achieved 100 million weekly active users by November 2023—less than one year after release [3]. The platform's weekly active user base has been doubling every 7-8 months since launch, with message volume increasing 5.8 times in the final year studied while user volume grew 3.2 times [3].\n",
       "\n",
       "This growth pattern demonstrates not merely viral adoption but sustained engagement, with all signup cohorts showing similar patterns of increased usage beginning in late 2024/early 2025. This suggests that improvements in ChatGPT's quality and user-friendliness, rather than mere novelty effects, drive the continued expansion [3].\n",
       "\n",
       "### Demographic Transformation and Patterns\n",
       "\n",
       "**Gender Gap Evolution**: One of the most significant findings concerns the dramatic closure of the initial gender gap. Early adopters were disproportionately male, reflecting typical patterns seen with new technologies. However, by July 2025, this gap had not only closed but slightly reversed, with 52% of active users having typically female first names compared to just 37% in January 2024 [2][6]. This represents one of the fastest gender parity achievements documented for a major technology platform.\n",
       "\n",
       "**Age Demographics**: The platform demonstrates strong appeal among younger users, with nearly half of all messages originating from users under 26 years old [4]. Users aged 18-24 form the primary demographic, with the 25-34 age group comprising the second largest segment. Combined, users between 18 and 34 represent 54.85% of the total user base [5], indicating ChatGPT's particular resonance with digital natives and early-career professionals.\n",
       "\n",
       "**Global Distribution and Economic Patterns**: The study documents fascinating patterns in global adoption that challenge conventional assumptions about technology diffusion. While the United States accounts for the largest single user base at 19.01%, this is followed by India (7.86%), Brazil (5.05%), Canada (3.57%), and the United Kingdom (3.48%) [5]. More significantly, growth rates in lower-income countries exceeded those in high-income nations by more than four times [6], suggesting that ChatGPT's value proposition may be particularly compelling in contexts where access to information and decision-support tools has traditionally been more limited.\n",
       "\n",
       "**Educational and Professional Characteristics**: Work usage correlates strongly with educational attainment and professional status, being more common among educated users in highly-paid professional occupations [2][4]. This pattern suggests that while ChatGPT has achieved broad demographic penetration, its workplace applications remain concentrated among knowledge workers who can most readily integrate AI assistance into their professional workflows.\n",
       "\n",
       "### User Engagement and Satisfaction Metrics\n",
       "\n",
       "User satisfaction remains consistently high throughout the study period, with positive interactions outnumbering negative ones by approximately 4:1 [4]. Users demonstrate substantial engagement, spending an average of 13 minutes and 58 seconds per session [7]. This engagement depth, combined with the frequency of use, indicates that ChatGPT has successfully transitioned from a novelty tool to an integrated component of users' digital workflows and daily routines.\n",
       "\n",
       "## Most Common Use Cases and Application Categories\n",
       "\n",
       "### The Three-Pillar Framework\n",
       "\n",
       "The study's most striking finding regarding use cases is the dominance of three primary categories: \"Practical Guidance,\" \"Seeking Information,\" and \"Writing,\" which collectively account for nearly 80% of all ChatGPT conversations [1][2][4][6]. This concentration suggests that despite the technology's broad capabilities, user behavior has converged around specific high-value applications.\n",
       "\n",
       "The researchers employed multiple classification frameworks to understand usage patterns. One approach categorizes interactions into three types: \"Asking\" (approximately 49% of messages), \"Doing\" (approximately 40%), and \"Expressing\" (approximately 11%) [4][6]. This framework reveals that ChatGPT primarily serves as an information and task-completion tool rather than a creative or expressive medium.\n",
       "\n",
       "### Writing as the Dominant Work Application\n",
       "\n",
       "Writing emerges as the single most important work-related application, dominating professional use cases and accounting for 40% of all work-related messages [6]. This finding highlights ChatGPT's unique value proposition compared to traditional search engines: its ability to generate tailored, contextual digital outputs rather than simply retrieving existing information. The prominence of writing applications spans various professional contexts, from email composition and document drafting to creative content generation and editing assistance.\n",
       "\n",
       "This writing dominance reflects a fundamental shift in how knowledge workers approach content creation, with ChatGPT serving as both a writing partner and a cognitive amplifier that can help users overcome writer's block, improve clarity, and generate ideas more efficiently.\n",
       "\n",
       "### Surprising Findings About Programming Usage\n",
       "\n",
       "Contrary to widespread perceptions about AI coding assistance, the study reveals that computer programming represents only 4.2% of all messages [4][6]. This finding challenges popular narratives about ChatGPT's primary value being in software development and suggests that mainstream adoption has moved far beyond technical use cases. The relatively small share of programming-related queries indicates that ChatGPT's broader utility lies in its natural language processing capabilities rather than specialized technical functions.\n",
       "\n",
       "### Information Seeking and Decision Support\n",
       "\n",
       "The study emphasizes ChatGPT's role as a decision-support tool, which emerges as its strongest economic value proposition [4][6]. Users frequently engage ChatGPT not merely to retrieve information but to help process, interpret, and apply that information to specific decisions or problems. This application proves especially valuable in knowledge-intensive jobs where workers must synthesize complex information and make nuanced judgments.\n",
       "\n",
       "The decision-support function distinguishes ChatGPT from traditional search engines by providing personalized, contextual advice rather than generic information retrieval. Users can engage in iterative conversations that help them think through problems, weigh options, and arrive at more informed conclusions.\n",
       "\n",
       "## Trends and Patterns from the Data\n",
       "\n",
       "### The Great Shift: From Work to Personal Use\n",
       "\n",
       "Perhaps the most significant trend documented in the study is the dramatic evolution from work-focused to personal usage. The data shows that non-work-related messages grew from 53% of all usage in mid-2024 to over 70% by mid-2025 [1][2][6]. This shift represents more than a gradual change; researchers describe it as a \"pirouetting\" rather than a flattening curve, indicating accelerating adoption for personal applications.\n",
       "\n",
       "This trend suggests that ChatGPT's initial adoption among professionals and early adopters has given way to mainstream consumer integration. As users become more comfortable with the technology and discover its utility for personal tasks—from planning activities and seeking advice to learning new topics and solving everyday problems—the platform has evolved into a general-purpose digital assistant rather than primarily a workplace tool.\n",
       "\n",
       "### Temporal Patterns and Engagement Evolution\n",
       "\n",
       "The study documents remarkable growth in both user numbers and message volume. Daily messages increased six-fold from 451 million to 2.6 billion within a single year, with 700 million people now sending approximately 18 billion messages weekly [6]. This growth pattern indicates not just user acquisition but deepening engagement as existing users find more applications for the technology.\n",
       "\n",
       "All signup cohorts show similar patterns of increased usage beginning in late 2024/early 2025, suggesting that platform improvements rather than cohort effects drive continued growth [3]. This pattern indicates that ChatGPT has substantially improved in quality and user-friendliness, creating value that transcends the initial novelty factor.\n",
       "\n",
       "### Geographic and Economic Expansion\n",
       "\n",
       "The study reveals fascinating patterns of global adoption that challenge traditional technology diffusion models. Countries with vastly different GDP per capita levels—including Brazil, South Korea, and the United States—now show similar adoption rates [3]. This convergence suggests that ChatGPT's value proposition transcends economic barriers in ways that previous technologies have not.\n",
       "\n",
       "The finding that growth rates in lower-income countries exceed those in high-income nations by more than four times [6] indicates that ChatGPT may be particularly valuable in contexts where access to information, education, and professional services has traditionally been limited. This pattern could have significant implications for global knowledge equity and economic development.\n",
       "\n",
       "### Demographic Maturation and Mainstream Integration\n",
       "\n",
       "The closure of the gender gap represents more than statistical parity; it indicates ChatGPT's successful transition from a male-dominated early adopter technology to a mainstream tool with broad demographic appeal. The shift from 37% female users in January 2024 to 52% by July 2025 [6] demonstrates rapid mainstream adoption among user groups who were initially underrepresented.\n",
       "\n",
       "This demographic maturation coincides with the platform's evolution from a work-focused tool to a comprehensive personal assistant. As use cases expanded beyond professional applications to include personal guidance, learning, entertainment, and daily problem-solving, the user base naturally diversified to reflect broader population demographics.\n",
       "\n",
       "### Value Creation and Economic Impact\n",
       "\n",
       "The researchers conclude that ChatGPT's primary economic value lies in its function as a decision-support tool that helps users make choices, think through problems, and produce better writing [4][6]. This value proposition distinguishes ChatGPT from traditional search engines through its ability to generate tailored, actionable outputs rather than simply retrieving information.\n",
       "\n",
       "The study suggests that ChatGPT generates significant consumer surplus from non-work applications, indicating substantial value creation beyond measurable productivity gains [6]. This consumer surplus reflects the platform's ability to enhance daily life through improved decision-making, learning opportunities, creative assistance, and problem-solving support.\n",
       "\n",
       "### The Mainstream Transformation\n",
       "\n",
       "The research documents ChatGPT's complete transformation from novelty to necessity. As one analysis notes, \"ChatGPT has gone mainstream. It's not just a toy or a novelty anymore—it's becoming part of daily routines, both at work and at home\" [8]. This integration into daily routines represents a fundamental shift in human-AI interaction, with ChatGPT serving as both a productivity tool and a cognitive companion.\n",
       "\n",
       "The platform's strongest appeal appears to be its function as a writing partner and sounding board, contributing to both productivity and creativity at an extraordinary scale [8]. This dual role—enhancing both efficiency and creative capability—positions ChatGPT as a transformative technology that augments human capabilities rather than simply automating existing tasks.\n",
       "\n",
       "The study's findings collectively suggest that we are witnessing the early stages of a fundamental shift in how humans interact with information, make decisions, and approach complex tasks. ChatGPT's rapid mainstream adoption and evolving use patterns indicate that AI assistants are becoming integral components of modern cognitive workflows, with implications that extend far beyond the current use cases documented in this groundbreaking research.\n",
       "\n",
       "### Sources\n",
       "\n",
       "[1] How People Use ChatGPT: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5487080\n",
       "[2] How People Use ChatGPT | NBER: https://www.nber.org/papers/w34255\n",
       "[3] How People Use ChatGPT - by David Deming: https://forklightning.substack.com/p/how-people-use-chatgpt\n",
       "[4] How People Really Use ChatGPT: Findings from NBER Research: https://techmaniacs.com/2025/09/15/how-people-really-use-chatgpt-findings-from-nber-research/\n",
       "[5] 33 Essential ChatGPT Statistics You Need To Know In 2025: https://thesocialshepherd.com/blog/chatgpt-statistics\n",
       "[6] How people use ChatGPT: Study reveals surprising trends: https://www.linkedin.com/posts/andrewbirmingham1_how-do-people-really-use-chatgpt-activity-7377472211877687297--wSm\n",
       "[7] Latest ChatGPT Statistics: 800M+ Users, Revenue (Oct 2025): https://nerdynav.com/chatgpt-statistics/\n",
       "[8] How People Are Really Using ChatGPT - Mike Jeffs: https://mikejeffs.com/blog/how-people-are-really-using-chatgpt/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Research workflow completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create our research request with PDF context\n",
    "research_request = f\"\"\"\n",
    "I have a PDF document about how people use AI. Please analyze this document and provide insights about:\n",
    "\n",
    "1. What are the main findings about how people are using AI?\n",
    "2. What are the most common use cases?\n",
    "3. What trends or patterns emerge from the data?\n",
    "\n",
    "Here's a short excerpt of the PDF content (truncated to reduce tokens):\n",
    "\n",
    "{pdf_content[:3000]}  # First 3k chars to stay within limits\n",
    "\n",
    "...[content truncated for context window]\n",
    "\"\"\"\n",
    "\n",
    "# Execute the graph\n",
    "async def run_research():\n",
    "    \"\"\"Run the research workflow and display results.\"\"\"\n",
    "    print(\"Starting research workflow...\\n\")\n",
    "    \n",
    "    async for event in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": research_request}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        # Display each step\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Node: {node_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if node_name == \"clarify_with_user\":\n",
    "                if \"messages\" in node_output:\n",
    "                    last_msg = node_output[\"messages\"][-1]\n",
    "                    print(f\"\\n{last_msg.content}\")\n",
    "            \n",
    "            elif node_name == \"write_research_brief\":\n",
    "                if \"research_brief\" in node_output:\n",
    "                    print(f\"\\nResearch Brief Generated:\")\n",
    "                    print(f\"{node_output['research_brief'][:500]}...\")\n",
    "            \n",
    "            elif node_name == \"supervisor\":\n",
    "                print(f\"\\nSupervisor planning research strategy...\")\n",
    "                if \"supervisor_messages\" in node_output:\n",
    "                    last_msg = node_output[\"supervisor_messages\"][-1]\n",
    "                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "                        print(f\"Tool calls: {len(last_msg.tool_calls)}\")\n",
    "                        for tc in last_msg.tool_calls:\n",
    "                            print(f\"  - {tc['name']}\")\n",
    "            \n",
    "            elif node_name == \"supervisor_tools\":\n",
    "                print(f\"\\nExecuting supervisor's tool calls...\")\n",
    "                if \"notes\" in node_output:\n",
    "                    print(f\"Research notes collected: {len(node_output['notes'])}\")\n",
    "            \n",
    "            elif node_name == \"final_report_generation\":\n",
    "                if \"final_report\" in node_output:\n",
    "                    print(f\"\\n\" + \"=\"*60)\n",
    "                    print(\"FINAL REPORT GENERATED\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "                    display(Markdown(node_output[\"final_report\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Research workflow completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the research\n",
    "await run_research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "Let's break down what happened:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "The system checked if your request was clear. Since you provided a PDF and specific questions, it likely proceeded without clarification.\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "Your request was transformed into a detailed research brief that guides the supervisor's delegation strategy.\n",
    "\n",
    "### Phase 3: Supervisor Delegation\n",
    "The supervisor analyzed the brief and decided how to break down the research:\n",
    "- Used `think_tool` to plan strategy\n",
    "- Called `ConductResearch` multiple times to delegate to parallel researchers\n",
    "- Each delegation specified a focused research topic\n",
    "\n",
    "### Phase 4: Parallel Research\n",
    "Multiple researchers worked simultaneously:\n",
    "- Each researcher used web search tools to gather information\n",
    "- Used `think_tool` to reflect after each search\n",
    "- Decided when they had enough information\n",
    "- Compressed their findings into clean summaries\n",
    "\n",
    "### Phase 5: Final Report\n",
    "All research findings were synthesized into a comprehensive report with:\n",
    "- Well-structured sections\n",
    "- Inline citations\n",
    "- Sources listed at the end\n",
    "- Balanced coverage of all findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🏗️ Activity #1: Try Different Configurations\n",
    "\n",
    "You can experiment with different settings to see how they affect the research.  You may select three or more of the following settings (or invent your own experiments) and describe the results.\n",
    "\n",
    "### Increase Parallelism\n",
    "```python\n",
    "\"max_concurrent_research_units\": 10  # More researchers working simultaneously\n",
    "```\n",
    "\n",
    "### Deeper Research\n",
    "```python\n",
    "\"max_researcher_iterations\": 8   # Supervisor can delegate more times\n",
    "\"max_react_tool_calls\": 15      # Each researcher can search more\n",
    "```\n",
    "\n",
    "### Use Anthropic Native Search\n",
    "```python\n",
    "\"search_api\": \"anthropic\"  # Use Claude's built-in web search\n",
    "```\n",
    "\n",
    "### Disable Clarification\n",
    "```python\n",
    "\"allow_clarification\": False  # Skip clarification phase\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Answer Activity #1 — Experiment Results\n",
    "I toggled some configs here and there. I wil base my results off of thes toggles. I hit some rate limits frequently, so I made some changes, but these summarizations are what I found:\n",
    "\n",
    "- Increase Parallelism with max_concurrent_research_units\n",
    "  - Reduced time by running researchers concurrently, but increased tokens\n",
    "  - Higher cost due to multiple simultaneous tool/model calls.\n",
    "\n",
    "- Deeper Research with max_researcher_iterations and max_react_tool_calls\n",
    "  - Produced better notes and a more comprehensive final report in my opinion\n",
    "  - Increased latency and token usage\n",
    "\n",
    "- Use Anthropic for search api\n",
    "  - Fewer tool calls, but had comparable quality in my opinion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Architecture Benefits\n",
    "1. **Dynamic Decomposition** - Research structure emerges from the question, not predefined\n",
    "2. **Parallel Efficiency** - Multiple researchers work simultaneously\n",
    "3. **ReAct Quality** - Strategic reflection improves search decisions\n",
    "4. **Scalability** - Handles token limits gracefully through compression\n",
    "5. **Flexibility** - Easy to add new tools and capabilities\n",
    "\n",
    "### When to Use This Pattern\n",
    "- **Complex research questions** that need multi-angle investigation\n",
    "- **Comparison tasks** where parallel research on different topics is beneficial\n",
    "- **Open-ended exploration** where structure should emerge dynamically\n",
    "- **Time-sensitive research** where parallel execution speeds up results\n",
    "\n",
    "### When to Use Section-Based Instead\n",
    "- **Highly structured reports** with predefined format requirements\n",
    "- **Template-based content** where sections are always the same\n",
    "- **Sequential dependencies** where later sections depend on earlier ones\n",
    "- **Budget constraints** where token efficiency is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Extend the System\n",
    "1. **Add MCP Tools** - Integrate specialized tools for your domain\n",
    "2. **Custom Prompts** - Modify prompts for specific research types\n",
    "3. **Different Models** - Try different Claude versions or mix models\n",
    "4. **Persistence** - Use a real database for checkpointing instead of memory\n",
    "\n",
    "### Learn More\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Open Deep Research Repo](https://github.com/langchain-ai/open_deep_research)\n",
    "- [Anthropic Claude Documentation](https://docs.anthropic.com/)\n",
    "- [Tavily Search API](https://tavily.com/)\n",
    "\n",
    "### Deploy\n",
    "- Use LangGraph Cloud for production deployment\n",
    "- Add proper error handling and logging\n",
    "- Implement rate limiting and cost controls\n",
    "- Monitor research quality and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
